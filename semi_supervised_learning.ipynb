{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from helper_methods import get_train_val_mltclass, get_test, create_dataset_mltclass, show_images, load_and_transform_image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Unzipping the annotations file\n",
    "annotations_tar_path = '/content/drive/MyDrive/annotations.tar.gz'\n",
    "annotations_extract_path = '/content/annotations'\n",
    "with tarfile.open(annotations_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=annotations_extract_path)\n",
    "\n",
    "# Unzipping the images file\n",
    "images_tar_path = '/content/drive/MyDrive/images.tar.gz'\n",
    "images_extract_path = '/content/images'\n",
    "with tarfile.open(images_tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=images_extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = get_train_val_mltclass(filepath='/content/annotations/annotations/trainval.txt', val_size=0.2)\n",
    "X_train, Y_train = create_dataset_mltclass(df_train, base_path = \"/content/images/images/\", augment=True)\n",
    "X_val, Y_val = create_dataset_mltclass(df_val, base_path=\"/content/images/images/\", augment = False)\n",
    "\n",
    "# Convert Y_train and Y_val to Long right after their creation\n",
    "Y_train = Y_train.long()\n",
    "Y_val = Y_val.long()\n",
    "\n",
    "\n",
    "#test data\n",
    "df_test = get_test(filepath='/content/annotations/annotations/test.txt')\n",
    "X_test, Y_test = create_dataset_mltclass(df_test, base_path=\"/content/images/images/\")\n",
    "\n",
    "# Convert Y_test to Long\n",
    "Y_test = Y_test.long()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        #print(\"TRAINING DEVIDE: \", device)\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch):\n",
    "    #     images, labels = batch\n",
    "    #     images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "    #     out = self(images)\n",
    "    #     with torch.no_grad():\n",
    "    #         loss = F.cross_entropy(out, labels)\n",
    "    #         acc = accuracy(out, labels)\n",
    "\n",
    "    #     return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "      images, labels = batch\n",
    "      #print(\"VALIDATION DEVICE: \", device)\n",
    "      images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "      out = self(images)\n",
    "\n",
    "      # Compute loss\n",
    "      loss = F.cross_entropy(out, labels)\n",
    "\n",
    "      # Compute number of correct predictions\n",
    "      _, predicted = torch.max(out, 1)\n",
    "      num_correct = (predicted == labels).sum().item()\n",
    "\n",
    "      return {'val_loss': loss.detach(), 'num_correct': num_correct}\n",
    "\n",
    "    def predict(self, images):\n",
    "      #images, _ = batch\n",
    "      images = images.to(device)\n",
    "      out = self(images)\n",
    "      _, predicted = torch.max(out, 1)\n",
    "      return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def set_bn_eval(module):\n",
    "    \"\"\"Set BatchNorm layers in evaluation mode.\"\"\"\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        module.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsCatsCnnModelResNet34(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        #self.network.fc = nn.Linear(num_ftrs, 37)\n",
    "\n",
    "        self.network.fc = nn.Sequential(\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(num_ftrs, 37)\n",
    "                      )\n",
    "\n",
    "        # Freeze the early layers\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if 'layer1' in name or 'layer2' in name or 'layer3' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "                # Apply the batch normalization adjustment\n",
    "                if isinstance(param, nn.BatchNorm2d):\n",
    "                    set_bn_eval(param)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return torch.sigmoid(self.network(xb))\n",
    "\n",
    "    def predict(self, images):\n",
    "      return super().predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#@torch.no_grad()\n",
    "def evaluate(model, X_val, Y_val, batch_size=256):\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    # Try loadign data into batches\n",
    "    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "      for batch in val_loader:\n",
    "        X_batch, Y_batch = batch\n",
    "        #X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)  # Move batch data to the GPU\n",
    "        outputs = model.validation_step([X_batch, Y_batch])\n",
    "        loss += outputs['val_loss'].item() * len(batch[0])\n",
    "        correct += outputs['num_correct']\n",
    "        total_samples += len(batch[0])\n",
    "\n",
    "    return {'val_loss': loss / total_samples, 'accuracy': correct / total_samples}\n",
    "    #batch = [X_val.to(device), Y_val.to(device)]  # Move batch data to the GPU\n",
    "    #outputs = model.validation_step(batch)\n",
    "    #return outputs\n",
    "\n",
    "def fit(model, epochs, lr, X_train, Y_train, X_val, Y_val, batch_size=32, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    # optimizer = opt_func(model.parameters(), lr)\n",
    "\n",
    "    # Filter out the parameters to update to ensure that \"Only Unfrozen Parameters Are Passed to the Optimizer\"\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = opt_func(params_to_update, lr)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "    # Create TensorDataset and DataLoader for training data\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #, num_workers=4\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    model = model.to(device)\n",
    "    #print(\"MODEL TO DEVICE: \", device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            X_batch, Y_batch = batch\n",
    "            #X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)  # Move batch data to the GPU\n",
    "\n",
    "            loss = model.training_step([X_batch, Y_batch])\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, X_val, Y_val, batch_size)\n",
    "        print(f\"Epoch {epoch}, result: {result}\")\n",
    "        history.append(result)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_resnet_34 = DogsCatsCnnModelResNet34().to(device)\n",
    "model_resnet_34 = DogsCatsCnnModelResNet34()\n",
    "#print(evaluate(model_resnet_34, X_val, Y_val,batch_size=256))\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "\n",
    "history0 = fit(model_resnet_34, num_epochs, lr, X_train, Y_train, X_val, Y_val, batch_size, opt_func)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Evaluation of model trained on the entire dataset\", evaluate(model_resnet_34, X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 100%, 50%, 10% and 1% of training data\n",
    "print(X_train.shape)\n",
    "# 50 %\n",
    "X_train_half = X_train[:len(X_train) // 2]\n",
    "Y_train_half = Y_train[:len(Y_train) // 2]\n",
    "X_unlabeled_half = X_train[len(X_train) // 2:]\n",
    "Y_unlabeled_half = Y_train[len(Y_train) // 2:]\n",
    "print(X_train_half.shape)\n",
    "print(Y_train_half.shape)\n",
    "print(X_unlabeled_half.shape)\n",
    "print(Y_unlabeled_half.shape)\n",
    "# 10 %\n",
    "X_train_ten = X_train[:len(X_train) // 10]\n",
    "Y_train_ten = Y_train[:len(Y_train) // 10]\n",
    "X_unlabeled_ten = X_train[len(X_train) // 10:]\n",
    "Y_unlabeled_ten = Y_train[len(Y_train) // 10:]\n",
    "print(X_train_ten.shape)\n",
    "print(Y_train_ten.shape)\n",
    "print(X_unlabeled_ten.shape)\n",
    "print(Y_unlabeled_ten.shape)\n",
    "# 1 %\n",
    "X_train_one = X_train[:len(X_train) // 100]\n",
    "Y_train_one = Y_train[:len(Y_train) // 100]\n",
    "X_unlabeled_one = X_train[len(X_train) // 100:]\n",
    "Y_unlabeled_one = Y_train[len(Y_train) // 100:]\n",
    "print(X_train_one.shape)\n",
    "print(Y_train_one.shape)\n",
    "print(X_unlabeled_one.shape)\n",
    "print(Y_unlabeled_one.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsCatsCnnModelResNet18(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        #self.network.fc = nn.Linear(num_ftrs, 37)\n",
    "\n",
    "        self.network.fc = nn.Sequential(\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(num_ftrs, 37)\n",
    "                      )\n",
    "\n",
    "        # Freeze the early layers\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if 'layer1' in name or 'layer2' in name or 'layer3' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "                # Apply the batch normalization adjustment\n",
    "                if isinstance(param, nn.BatchNorm2d):\n",
    "                    set_bn_eval(param)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return torch.sigmoid(self.network(xb))\n",
    "\n",
    "    def predict(self, images):\n",
    "      return super().predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use ResNet18 as our teacher model\n",
    "#model_resnet_18_half = DogsCatsCnnModelResNet18().to(device)\n",
    "model_resnet_18_half_teacher = DogsCatsCnnModelResNet18()\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "history_half = fit(model_resnet_18_half_teacher, num_epochs, lr, X_train_half, Y_train_half, X_val, Y_val, batch_size, opt_func)\n",
    "evaluate_teacher_half = evaluate(model_resnet_18_half_teacher, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 50% of dataset\", evaluate_teacher_half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(X_unlabeled_half, Y_unlabeled_half)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(data_loader):\n",
    "        images, _ = batch\n",
    "        predictions.append(model_resnet_18_half_teacher.predict(images))\n",
    "#print(predictions)\n",
    "flattened_predictions = [prediction.flatten() for prediction in predictions]\n",
    "Y_unlabeled_half_predictions = torch.cat(flattened_predictions)\n",
    "print(len(Y_unlabeled_half_predictions))\n",
    "print(len(Y_unlabeled_half))\n",
    "#print(Y_unlabeled_half_predictions[1])\n",
    "#print(Y_unlabeled_half[1])\n",
    "Y_unlabeled_half = Y_unlabeled_half.to(device)\n",
    "#print(predictions_flat[1])\n",
    "#print(Y_unlabeled_half[1])\n",
    "acc = (Y_unlabeled_half_predictions == Y_unlabeled_half).sum().item() / len(Y_unlabeled_half)\n",
    "print(\"Accuracy: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataset with the pseudo labels so that we can train the student model in the entire dataset\n",
    "\n",
    "\n",
    "X_train_half_concatenated =  torch.cat((X_train_half, X_unlabeled_half), dim=0)\n",
    "Y_train_half_concatenated = torch.cat((Y_train_half.to(device), Y_unlabeled_half_predictions), dim=0)\n",
    "\n",
    "model_resnet_18_half_student = DogsCatsCnnModelResNet18()\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device)\n",
    "fit(model_resnet_18_half_student, num_epochs, lr, X_train_half_concatenated, Y_train_half_concatenated, X_val, Y_val, batch_size, opt_func)\n",
    "evaluate_student_half = evaluate(model_resnet_18_half_student, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 50% of dataset\", evaluate_student_half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Delete variables\n",
    "del evaluate_student_half, model_resnet_18_half_student, X_train_half_concatenated, Y_train_half_concatenated\n",
    "del X_train_half, X_unlabeled_half, Y_train_half, Y_unlabeled_half, Y_unlabeled_half_predictions, flattened_predictions\n",
    "del model_resnet_18_half_teacher, model_resnet_34, X_train, Y_train\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_18_ten_teacher = DogsCatsCnnModelResNet18()\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "fit(model_resnet_18_ten_teacher, num_epochs, lr, X_train_ten, Y_train_ten, X_val, Y_val, batch_size, opt_func)\n",
    "\n",
    "evaluate_teacher_ten = evaluate(model_resnet_18_ten_teacher, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 10% of dataset\", evaluate_teacher_ten)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(X_unlabeled_ten, Y_unlabeled_ten)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4) #, num_workers=4\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(data_loader):\n",
    "        images, _ = batch\n",
    "        predictions.append(model_resnet_18_ten_teacher.predict(images))\n",
    "#print(predictions)\n",
    "\n",
    "flattened_predictions = [prediction.flatten() for prediction in predictions]\n",
    "Y_unlabeled_ten_predictions = torch.cat(flattened_predictions)\n",
    "print(len(Y_unlabeled_ten_predictions))\n",
    "print(len(Y_unlabeled_ten))\n",
    "\n",
    "Y_unlabeled_ten = Y_unlabeled_ten.to(device)\n",
    "#print(predictions_flat[1])\n",
    "#print(Y_unlabeled_half[1])\n",
    "acc = (Y_unlabeled_ten_predictions == Y_unlabeled_ten).sum().item() / len(Y_unlabeled_ten)\n",
    "print(\"Accuracy: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataset with the pseudo labels so that we can train the student model in the entire dataset\n",
    "\n",
    "X_train_ten_concatenated =  torch.cat((X_train_ten, X_unlabeled_ten), dim=0)\n",
    "Y_train_ten_concatenated = torch.cat((Y_train_ten.to(device), Y_unlabeled_ten_predictions), dim=0)\n",
    "\n",
    "model_resnet_18_ten_student = DogsCatsCnnModelResNet18()\n",
    "\n",
    "fit(model_resnet_18_ten_student, num_epochs, lr, X_train_ten_concatenated, Y_train_ten_concatenated, X_val, Y_val, batch_size, opt_func)\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "evaluate_student_ten= evaluate(model_resnet_18_ten_student, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 10% of dataset\", evaluate_student_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Delete variables\n",
    "del evaluate_student_ten, model_resnet_18_ten_student, X_train_ten_concatenated, Y_train_ten_concatenated\n",
    "del X_train_ten, X_unlabeled_ten, Y_train_ten, Y_unlabeled_ten, Y_unlabeled_ten_predictions, flattened_predictions\n",
    "del model_resnet_18_ten_teacher\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_18_one_teacher = DogsCatsCnnModelResNet18()\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "fit(model_resnet_18_one_teacher, num_epochs, lr, X_train_one, Y_train_one, X_val, Y_val, batch_size, opt_func)\n",
    "\n",
    "evaluate_teacher_one = evaluate(model_resnet_18_one_teacher, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 1% of dataset\", evaluate_teacher_one)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(X_unlabeled_one, Y_unlabeled_one)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4) #, num_workers=4\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(data_loader):\n",
    "        images, _ = batch\n",
    "        predictions.append(model_resnet_18_one_teacher.predict(images))\n",
    "#print(predictions)\n",
    "\n",
    "flattened_predictions = [prediction.flatten() for prediction in predictions]\n",
    "Y_unlabeled_one_predictions = torch.cat(flattened_predictions)\n",
    "print(len(Y_unlabeled_one_predictions))\n",
    "print(len(Y_unlabeled_one))\n",
    "\n",
    "Y_unlabeled_one = Y_unlabeled_one.to(device)\n",
    "#print(predictions_flat[1])\n",
    "#print(Y_unlabeled_half[1])\n",
    "acc = (Y_unlabeled_one_predictions == Y_unlabeled_one).sum().item() / len(Y_unlabeled_one)\n",
    "print(\"Accuracy: \", acc)\n",
    "# Concatenate the dataset with the pseudo labels so that we can train the student model in the entire dataset\n",
    "\n",
    "X_train_one_concatenated =  torch.cat((X_train_one, X_unlabeled_one), dim=0)\n",
    "Y_train_one_concatenated = torch.cat((Y_train_one.to(device), Y_unlabeled_one_predictions), dim=0)\n",
    "\n",
    "model_resnet_18_one_student = DogsCatsCnnModelResNet18()\n",
    "\n",
    "fit(model_resnet_18_one_student, num_epochs, lr, X_train_one_concatenated, Y_train_one_concatenated, X_val, Y_val, batch_size, opt_func)\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "evaluate_student_one= evaluate(model_resnet_18_one_student, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 1% of dataset\", evaluate_student_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Delete variables\n",
    "del evaluate_student_one, model_resnet_18_one_student, X_train_one_concatenated, Y_train_one_concatenated\n",
    "del X_train_one, X_unlabeled_one, Y_train_one, Y_unlabeled_one, Y_unlabeled_one_predictions, flattened_predictions\n",
    "del model_resnet_18_one_teacher\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "\n",
    "class DogsCatsCnnModelResNet101(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        #self.network.fc = nn.Linear(num_ftrs, 37)\n",
    "\n",
    "        self.network.fc = nn.Sequential(\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(num_ftrs, 37)\n",
    "                      )\n",
    "\n",
    "        # Freeze the early layers\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if 'layer1' in name or 'layer2' in name or 'layer3' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "                # Apply the batch normalization adjustment\n",
    "                if isinstance(param, nn.BatchNorm2d):\n",
    "                    set_bn_eval(param)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return torch.sigmoid(self.network(xb))\n",
    "\n",
    "    def predict(self, images):\n",
    "      return super().predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(model, epochs, lr, X_train, Y_train, X_val, Y_val, batch_size=32, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    # optimizer = opt_func(model.parameters(), lr)\n",
    "\n",
    "    # Filter out the parameters to update to ensure that \"Only Unfrozen Parameters Are Passed to the Optimizer\"\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = opt_func(params_to_update, lr)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "    # Create TensorDataset and DataLoader for training data\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #, num_workers=4\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    model = model.to(device)\n",
    "    #print(\"MODEL TO DEVICE: \", device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            X_batch, Y_batch = batch\n",
    "            #X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)  # Move batch data to the GPU\n",
    "\n",
    "            loss = model.training_step([X_batch, Y_batch])\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        stacked_tensors = torch.stack(train_losses)\n",
    "        # Sum along the first dimension (the new dimension)\n",
    "        total = torch.sum(stacked_tensors, dim=0)\n",
    "        print(f\"Epoch {epoch}, result: {total}\")\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, X_val, Y_val, batch_size)\n",
    "        print(f\"Epoch {epoch}, result: {result}\")\n",
    "        history.append(result)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet_101_one_teacher = DogsCatsCnnModelResNet101()\n",
    "num_epochs = 20\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "fit(model_resnet_101_one_teacher, num_epochs, lr, X_train_one, Y_train_one, X_val, Y_val, batch_size, opt_func)\n",
    "\n",
    "evaluate_teacher_one = evaluate(model_resnet_101_one_teacher, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 1% of dataset\", evaluate_teacher_one)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(X_unlabeled_one, Y_unlabeled_one)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False) #, num_workers=4\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(data_loader):\n",
    "        images, _ = batch\n",
    "        predictions.append(model_resnet_101_one_teacher.predict(images))\n",
    "#print(predictions)\n",
    "\n",
    "flattened_predictions = [prediction.flatten() for prediction in predictions]\n",
    "Y_unlabeled_one_predictions = torch.cat(flattened_predictions)\n",
    "print(len(Y_unlabeled_one_predictions))\n",
    "print(len(Y_unlabeled_one))\n",
    "\n",
    "Y_unlabeled_one = Y_unlabeled_one.to(device)\n",
    "#print(predictions_flat[1])\n",
    "#print(Y_unlabeled_half[1])\n",
    "acc = (Y_unlabeled_one_predictions == Y_unlabeled_one).sum().item() / len(Y_unlabeled_one)\n",
    "print(\"Accuracy: \", acc)\n",
    "# Concatenate the dataset with the pseudo labels so that we can train the student model in the entire dataset\n",
    "\n",
    "X_train_one_concatenated =  torch.cat((X_train_one, X_unlabeled_one), dim=0)\n",
    "Y_train_one_concatenated = torch.cat((Y_train_one.to(device), Y_unlabeled_one_predictions), dim=0)\n",
    "\n",
    "model_resnet_101_one_student = DogsCatsCnnModelResNet101()\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "fit(model_resnet_101_one_student, num_epochs, lr, X_train_one_concatenated, Y_train_one_concatenated, X_val, Y_val, batch_size, opt_func)\n",
    "\n",
    "evaluate_student_one= evaluate(model_resnet_101_one_student, X_test, Y_test)\n",
    "print(\"Evaluation of model trained on the 1% of dataset\", evaluate_student_one)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
